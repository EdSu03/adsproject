{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Template Starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.backends.cudnn\n",
    "import numpy as np\n",
    "import torchvision.datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%pip install jupyter_contrib_nbextensions\n",
    "!jupyter contrib nbextension install --user\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the data!\n",
    "\n",
    "# Toggle whether to use the whole data or just a sample\n",
    "USE_SAMPLE = True\n",
    "\n",
    "# Scrape the given URL for all parquet file links\n",
    "def get_parquets(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return [\n",
    "        a[\"href\"].strip()\n",
    "        for a in soup.find_all(\"a\", href=True)\n",
    "        if a[\"href\"].strip().endswith(\".parquet\")\n",
    "        and (not USE_SAMPLE or (\"yellow\" in a[\"href\"] and \"2024\" in a[\"href\"]))\n",
    "    ]\n",
    "\n",
    "# Download a single file while displaying a progress bar\n",
    "def download_file(url, save_path, position=1):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "    with open(save_path, 'wb') as file, tqdm(\n",
    "        desc=os.path.basename(save_path),\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "        position=position,\n",
    "        leave=False,\n",
    "        dynamic_ncols=True,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "\n",
    "# Download and process all parquet files found on the webpage\n",
    "def download_and_read_parquet_files(url, save_dir=\"data\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    links = get_parquets(url)\n",
    "    dataframes = []\n",
    "    \n",
    "    with tqdm(total=len(links), desc=\"Processing files\", unit=\"file\", position=0, leave=True) as file_bar:\n",
    "        for link in links:\n",
    "            full_link = urljoin(url, link)\n",
    "            filename = os.path.join(save_dir, os.path.basename(link))\n",
    "\n",
    "            if not os.path.exists(filename):\n",
    "                download_file(full_link, filename, position=1)\n",
    "                time.sleep(5)\n",
    "\n",
    "            attempt = 0\n",
    "            while attempt < 2:\n",
    "                try:\n",
    "                    df = pd.read_parquet(filename, engine='fastparquet')\n",
    "                    df[df.select_dtypes(include=['number']).columns] = df.select_dtypes(include=['number']).apply(pd.to_numeric, downcast='float')\n",
    "                    dataframes.append(df)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {filename}: {e}\")\n",
    "                    if attempt == 0:\n",
    "                        os.remove(filename)\n",
    "                        print(\"Redownloading...\")\n",
    "                        download_file(full_link, filename, position=1)\n",
    "                        time.sleep(5)\n",
    "                    attempt += 1\n",
    "            \n",
    "            file_bar.update(1)\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "dfs = download_and_read_parquet_files(url)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(combined_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
