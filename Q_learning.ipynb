{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T15:11:59.345504Z",
     "start_time": "2025-04-03T15:11:57.758170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from DOPU_given_timerange import get_combined_cleaned_one_month_df\n",
    "\n",
    "get_combined_cleaned_one_month_df()"
   ],
   "id": "2092f11157b06206",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T20:24:11.499033Z",
     "start_time": "2025-04-03T20:23:55.440549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from zone_coords import get_coord_as_csv\n",
    "# Required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# === Step 1: Load Parquet taxi trip data ===\n",
    "df = pd.read_parquet('./all_cleaned_data/combined_cleaned_data_2024-01.parquet')\n",
    "\n",
    "# === Step 2: Load zone coordinates ===\n",
    "# CSV should contain: LocationID, Latitude, Longitude\n",
    "get_coord_as_csv()\n",
    "zone_coords_df = pd.read_csv('zone_coords.csv')\n",
    "zone_coords = {\n",
    "    int(row['LocationID']): (row['Latitude'], row['Longitude'])\n",
    "    for _, row in zone_coords_df.iterrows()\n",
    "}\n",
    "\n",
    "# === Step 3: Load zone neighbor dictionary from JSON ===\n",
    "with open('zone_neighbors.json', 'r') as f:\n",
    "    neighbor_dict = json.load(f)\n",
    "\n",
    "# Convert string keys and values to integers\n",
    "neighbor_dict = {int(k): [int(n) for n in v] for k, v in neighbor_dict.items()}\n",
    "\n",
    "# === Step 4: Helper functions ===\n",
    "\n",
    "# Convert a timestamp into a 30-minute time bin index (0 to 47)\n",
    "def get_time_bin_half_hour(dt):\n",
    "    return dt.hour * 2 + dt.minute // 30\n",
    "\n",
    "\n",
    "# Convert a timestamp into a 15-minute time bin index (0 to 95)\n",
    "def get_time_bin_fifteen_minutes(timestamp):\n",
    "    return timestamp.hour * 4 + timestamp.minute // 15  # 15min per bin\n",
    "\n",
    "\n",
    "# Compute Euclidean distance between two (lat, lon) coordinates\n",
    "def euclidean_distance(coord1, coord2):\n",
    "    return np.linalg.norm(np.array(coord1) - np.array(coord2))\n",
    "\n",
    "# Compute reward: fare minus 0.5 * empty travel distance\n",
    "def compute_reward(fare, empty_distance):\n",
    "    return fare - 0.5 * empty_distance\n",
    "\n",
    "# === Step 5: Construct (s, a, r, sâ€²) tuples ===\n",
    "\n",
    "samples = []\n",
    "\n",
    "num_time_bins = 96\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        # Extract key fields\n",
    "        s_zone = int(row['PULocationID'])\n",
    "        pickup_time = pd.to_datetime(row['PickupDatetime'])\n",
    "        fare = float(row['FareAmount'])\n",
    "\n",
    "        # Skip invalid or missing data\n",
    "        if s_zone not in zone_coords or s_zone not in neighbor_dict:\n",
    "            continue\n",
    "\n",
    "        # Define current state\n",
    "        t_bin = get_time_bin_fifteen_minutes(pickup_time)\n",
    "        s = (s_zone, t_bin)\n",
    "\n",
    "        # Loop over each possible neighbor as an action\n",
    "        for a_zone in neighbor_dict[s_zone]:\n",
    "            if a_zone not in zone_coords:\n",
    "                continue\n",
    "\n",
    "            # Calculate distance between zones\n",
    "            dist = euclidean_distance(zone_coords[s_zone], zone_coords[a_zone])\n",
    "\n",
    "            # Calculate reward for moving to that zone\n",
    "            reward = compute_reward(fare, dist)\n",
    "\n",
    "            # Next state: new zone, 30/15 minutes later\n",
    "            s_prime = (a_zone, (t_bin + 1) % num_time_bins)\n",
    "\n",
    "            # Save sample\n",
    "            samples.append({\n",
    "                's_zone': s_zone,\n",
    "                't_bin': t_bin,\n",
    "                'a_zone': a_zone,\n",
    "                'reward': reward,\n",
    "                's_prime_zone': s_prime[0],\n",
    "                's_prime_t_bin': s_prime[1]\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        # Silently skip problematic rows\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "samples_df = pd.DataFrame(samples)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 73\u001B[0m\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;66;03m# Calculate distance between zones\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m dist \u001B[38;5;241m=\u001B[39m euclidean_distance(zone_coords[s_zone], zone_coords[a_zone])\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m# Calculate reward for moving to that zone\u001B[39;00m\n\u001B[0;32m     76\u001B[0m reward \u001B[38;5;241m=\u001B[39m compute_reward(fare, dist)\n",
      "Cell \u001B[1;32mIn[32], line 40\u001B[0m, in \u001B[0;36meuclidean_distance\u001B[1;34m(coord1, coord2)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21meuclidean_distance\u001B[39m(coord1, coord2):\n\u001B[1;32m---> 40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(np\u001B[38;5;241m.\u001B[39marray(coord1) \u001B[38;5;241m-\u001B[39m np\u001B[38;5;241m.\u001B[39marray(coord2))\n",
      "File \u001B[1;32mD:\\Environment\\anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2552\u001B[0m, in \u001B[0;36mnorm\u001B[1;34m(x, ord, axis, keepdims)\u001B[0m\n\u001B[0;32m   2550\u001B[0m     sqnorm \u001B[38;5;241m=\u001B[39m x_real\u001B[38;5;241m.\u001B[39mdot(x_real) \u001B[38;5;241m+\u001B[39m x_imag\u001B[38;5;241m.\u001B[39mdot(x_imag)\n\u001B[0;32m   2551\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2552\u001B[0m     sqnorm \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mdot(x)\n\u001B[0;32m   2553\u001B[0m ret \u001B[38;5;241m=\u001B[39m sqrt(sqnorm)\n\u001B[0;32m   2554\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m keepdims:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:28:11.147458Z",
     "start_time": "2025-04-04T12:28:06.092846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"./data_for_q_learning/q_learning_samples_96.csv\")\n",
    "df"
   ],
   "id": "5788b830bc4e3e0d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          s_zone  t_bin  a_zone     reward  s_prime_zone  s_prime_t_bin\n",
       "0            236      3     263  12.796885           263              4\n",
       "1            236      3      43  12.795609            43              4\n",
       "2            236      3     262  12.794287           262              4\n",
       "3            236      3     141  12.793130           141              4\n",
       "4            236      3     140  12.792438           140              4\n",
       "...          ...    ...     ...        ...           ...            ...\n",
       "13526230     140     92     141   8.597445           141             93\n",
       "13526231     140     92     202   8.597010           202             93\n",
       "13526232     140     92     237   8.594332           237             93\n",
       "13526233     140     92     263   8.593102           263             93\n",
       "13526234     140     92     236   8.592438           236             93\n",
       "\n",
       "[13526235 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_zone</th>\n",
       "      <th>t_bin</th>\n",
       "      <th>a_zone</th>\n",
       "      <th>reward</th>\n",
       "      <th>s_prime_zone</th>\n",
       "      <th>s_prime_t_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236</td>\n",
       "      <td>3</td>\n",
       "      <td>263</td>\n",
       "      <td>12.796885</td>\n",
       "      <td>263</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>12.795609</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>236</td>\n",
       "      <td>3</td>\n",
       "      <td>262</td>\n",
       "      <td>12.794287</td>\n",
       "      <td>262</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>236</td>\n",
       "      <td>3</td>\n",
       "      <td>141</td>\n",
       "      <td>12.793130</td>\n",
       "      <td>141</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>236</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>12.792438</td>\n",
       "      <td>140</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526230</th>\n",
       "      <td>140</td>\n",
       "      <td>92</td>\n",
       "      <td>141</td>\n",
       "      <td>8.597445</td>\n",
       "      <td>141</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526231</th>\n",
       "      <td>140</td>\n",
       "      <td>92</td>\n",
       "      <td>202</td>\n",
       "      <td>8.597010</td>\n",
       "      <td>202</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526232</th>\n",
       "      <td>140</td>\n",
       "      <td>92</td>\n",
       "      <td>237</td>\n",
       "      <td>8.594332</td>\n",
       "      <td>237</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526233</th>\n",
       "      <td>140</td>\n",
       "      <td>92</td>\n",
       "      <td>263</td>\n",
       "      <td>8.593102</td>\n",
       "      <td>263</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526234</th>\n",
       "      <td>140</td>\n",
       "      <td>92</td>\n",
       "      <td>236</td>\n",
       "      <td>8.592438</td>\n",
       "      <td>236</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13526235 rows Ã— 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T20:27:55.735945Z",
     "start_time": "2025-04-03T20:27:31.606828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"./Q_learning\", exist_ok=True)\n",
    "output_path = f'./data_for_q_learning/q_learning_samples_{num_time_bins}.csv'\n",
    "samples_df.to_csv(output_path, index=False)\n",
    "print(f\"âœ… Done! Generated {len(samples_df)} (s, a, r, sâ€²) samples and saved to {output_path}\")"
   ],
   "id": "21fff24c4a668d42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done! Generated 13526235 (s, a, r, sâ€²) samples and saved to ./data_for_q_learning/q_learning_samples_96.csv\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T22:01:26.977379Z",
     "start_time": "2025-04-03T20:28:21.899019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# === Step 1: Load sample data ===\n",
    "num_time_bins = 96 #15min time bin\n",
    "input_path = f'./data_for_q_learning/q_learning_samples_{num_time_bins}.csv'\n",
    "df = pd.read_csv(input_path)\n",
    "# Encode all zone IDs into integer IDs\n",
    "all_zone_ids = pd.unique(df[['s_zone', 'a_zone', 's_prime_zone']].values.ravel())\n",
    "zone_id_map = {z: i for i, z in enumerate(sorted(all_zone_ids))}\n",
    "num_zones = len(zone_id_map)\n",
    "\n",
    "\n",
    "# Map original zone IDs to encoded integers\n",
    "df['s_zone'] = df['s_zone'].map(zone_id_map)\n",
    "df['a_zone'] = df['a_zone'].map(zone_id_map)\n",
    "df['s_prime_zone'] = df['s_prime_zone'].map(zone_id_map)\n",
    "\n",
    "# === Step 2: Create PyTorch Dataset ===\n",
    "class QLearningDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.s_zone = torch.tensor(df['s_zone'].values, dtype=torch.long)\n",
    "        self.t_bin = torch.tensor(df['t_bin'].values, dtype=torch.long)\n",
    "        self.a_zone = torch.tensor(df['a_zone'].values, dtype=torch.long)\n",
    "        self.reward = torch.tensor(df['reward'].values, dtype=torch.float32)\n",
    "        self.s_prime_zone = torch.tensor(df['s_prime_zone'].values, dtype=torch.long)\n",
    "        self.s_prime_t_bin = torch.tensor(df['s_prime_t_bin'].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.s_zone)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.s_zone[idx], self.t_bin[idx], self.a_zone[idx],\n",
    "                self.reward[idx], self.s_prime_zone[idx], self.s_prime_t_bin[idx])\n",
    "\n",
    "dataset = QLearningDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# === Step 3: Q-network definition ===\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_zones, num_time_bins, embed_dim=16):\n",
    "        super().__init__()\n",
    "        self.zone_embed = nn.Embedding(num_zones, embed_dim)\n",
    "        self.time_embed = nn.Embedding(num_time_bins, embed_dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, s_zone, t_bin, a_zone):\n",
    "        s_embed = self.zone_embed(s_zone)\n",
    "        t_embed = self.time_embed(t_bin)\n",
    "        a_embed = self.zone_embed(a_zone)\n",
    "        x = torch.cat([s_embed, t_embed, a_embed], dim=1)\n",
    "        return self.net(x).squeeze(1)  # Output shape: (batch,)\n",
    "\n",
    "# Instantiate networks\n",
    "q_net = QNetwork(num_zones, num_time_bins)\n",
    "target_q_net = QNetwork(num_zones, num_time_bins)\n",
    "target_q_net.load_state_dict(q_net.state_dict())  # Copy initial weights\n",
    "\n",
    "# === Step 4: Training setup ===\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "# === Step 5: Training loop ===\n",
    "epoch_losses = []\n",
    "for epoch in range(5):\n",
    "    q_net.train()\n",
    "    total_loss = 0\n",
    "    tqdm_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for s, t, a, r, s_p, t_p in tqdm_bar:\n",
    "        # Compute target: r + Î³ * max_aâ€² Q(sâ€², aâ€²)\n",
    "        with torch.no_grad():\n",
    "            a_prime = torch.arange(num_zones).repeat(len(s_p), 1).to(torch.long)\n",
    "            s_prime_rep = s_p.unsqueeze(1).repeat(1, num_zones).flatten()\n",
    "            t_prime_rep = t_p.unsqueeze(1).repeat(1, num_zones).flatten()\n",
    "            a_prime_flat = a_prime.flatten()\n",
    "\n",
    "            q_values = target_q_net(s_prime_rep, t_prime_rep, a_prime_flat)\n",
    "            q_values = q_values.view(len(s_p), num_zones)\n",
    "            max_q = q_values.max(dim=1)[0]\n",
    "\n",
    "        # Use Bellman loss\n",
    "        target = r + gamma * max_q\n",
    "        pred_q = q_net(s, t, a)\n",
    "        loss = loss_fn(pred_q, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        tqdm_bar.set_postfix(batch_loss=loss.item())\n",
    "    \n",
    "    epoch_losses.append(total_loss)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# === Step 6: Save trained model and loss history ===\n",
    "torch.save(q_net.state_dict(), \"Q_learning/q_network.pt\")\n",
    "print(\"âœ… Q-network model saved to 'q_network.pt'\")\n",
    "\n",
    "# Save loss history\n",
    "pd.DataFrame({'epoch': list(range(1, len(epoch_losses)+1)), 'loss': epoch_losses})\\\n",
    "  .to_csv(\"./Q_learning/q_training_loss.csv\", index=False)\n",
    "print(\"ðŸ“„ Training loss saved to 'q_training_loss.csv'\")\n"
   ],
   "id": "4c0fe6c6d1319b22",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13210/13210 [18:17<00:00, 12.04it/s, batch_loss=66.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1348152.0779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13210/13210 [18:51<00:00, 11.67it/s, batch_loss=84.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1282722.9682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13210/13210 [18:03<00:00, 12.20it/s, batch_loss=62.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1280436.8355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13210/13210 [17:50<00:00, 12.34it/s, batch_loss=94.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1279344.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13210/13210 [19:55<00:00, 11.05it/s, batch_loss=82.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1278608.2782\n",
      "âœ… Q-network model saved to 'q_network.pt'\n",
      "ðŸ“„ Training loss saved to 'q_training_loss.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:23:48.668510Z",
     "start_time": "2025-04-04T12:23:41.205868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import random\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_zones, num_time_bins, embed_dim=16):\n",
    "        super().__init__()\n",
    "        self.zone_embed = nn.Embedding(num_zones, embed_dim)\n",
    "        self.time_embed = nn.Embedding(num_time_bins, embed_dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, s_zone, t_bin, a_zone):\n",
    "        s_embed = self.zone_embed(s_zone)\n",
    "        t_embed = self.time_embed(t_bin)\n",
    "        a_embed = self.zone_embed(a_zone)\n",
    "        x = torch.cat([s_embed, t_embed, a_embed], dim=1)\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "# === Load the trained model ===\n",
    "df = pd.read_csv(\"./data_for_q_learning/q_learning_samples_96.csv\")\n",
    "zone_ids = pd.unique(df[['s_zone', 'a_zone', 's_prime_zone']].values.ravel())\n",
    "zone_id_map = {z: i for i, z in enumerate(sorted(zone_ids))}\n",
    "inv_zone_map = {v: k for k, v in zone_id_map.items()}\n",
    "num_zones = len(zone_id_map)\n",
    "num_time_bins = 96\n",
    "\n",
    "df['s_zone'] = df['s_zone'].map(zone_id_map)\n",
    "df['a_zone'] = df['a_zone'].map(zone_id_map)\n",
    "\n",
    "# Calculate reward average table (s_zone, a_zone) â†’ avg_reward\n",
    "reward_lookup = df.groupby(['s_zone', 'a_zone'])['reward'].mean().to_dict()\n",
    "\n",
    "# Load adjacent zone information\n",
    "with open(\"zone_neighbors.json\", \"r\") as f:\n",
    "    neighbor_dict_raw = json.load(f)\n",
    "neighbor_dict = {zone_id_map[int(k)]: [zone_id_map[int(n)] for n in v if int(n) in zone_id_map] \n",
    "                 for k, v in neighbor_dict_raw.items() if int(k) in zone_id_map}\n",
    "\n",
    "# Initialize the model\n",
    "q_net = QNetwork(num_zones, num_time_bins)\n",
    "q_net.load_state_dict(torch.load(\"Q_learning/q_network.pt\"))\n",
    "q_net.eval()\n",
    "\n",
    "# === rollout Simulating functions ===\n",
    "def rollout(start_zone, start_time_bin, steps=32):\n",
    "    s_zone = zone_id_map[start_zone]\n",
    "    t_bin = start_time_bin\n",
    "    total_reward = 0\n",
    "    path = [(start_zone, t_bin)]\n",
    "\n",
    "    for _ in range(steps):\n",
    "        neighbors = neighbor_dict.get(s_zone, [])\n",
    "        if not neighbors:\n",
    "            break\n",
    "\n",
    "        s_tensor = torch.tensor([s_zone] * len(neighbors), dtype=torch.long)\n",
    "        t_tensor = torch.tensor([t_bin] * len(neighbors), dtype=torch.long)\n",
    "        a_tensor = torch.tensor(neighbors, dtype=torch.long)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(s_tensor, t_tensor, a_tensor)\n",
    "        best_idx = torch.argmax(q_values).item()\n",
    "        best_a = neighbors[best_idx]\n",
    "\n",
    "        reward = reward_lookup.get((s_zone, best_a), 0.0)\n",
    "        total_reward += reward\n",
    "\n",
    "        s_zone = best_a\n",
    "        t_bin = (t_bin + 1) % num_time_bins\n",
    "        path.append((inv_zone_map[s_zone], t_bin))\n",
    "\n",
    "    return total_reward, path\n",
    "\n",
    "# === Run a strategy simulation ===\n",
    "start_row = df.sample(1).iloc[0]\n",
    "start_zone = inv_zone_map[start_row['s_zone']]\n",
    "start_time = int(start_row['t_bin'])\n",
    "\n",
    "total_income, visited_path = rollout(start_zone, start_time)\n",
    "\n",
    "print(\"ðŸ§­ Simulated Path:\")\n",
    "for zone, t in visited_path:\n",
    "    print(f\"Zone {zone} at TimeBin {t}\")\n",
    "print(f\"\\nðŸ’° Total Estimated Income over {len(visited_path)-1} steps: ${total_income:.2f}\")\n"
   ],
   "id": "5cc62d19bb401dbd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiqiz\\AppData\\Local\\Temp\\ipykernel_25380\\3957463604.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  q_net.load_state_dict(torch.load(\"Q_learning/q_network.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§­ Simulated Path:\n",
      "Zone 186 at TimeBin 43\n",
      "Zone 100 at TimeBin 44\n",
      "Zone 164 at TimeBin 45\n",
      "Zone 107 at TimeBin 46\n",
      "Zone 224 at TimeBin 47\n",
      "Zone 4 at TimeBin 48\n",
      "Zone 224 at TimeBin 49\n",
      "Zone 79 at TimeBin 50\n",
      "Zone 107 at TimeBin 51\n",
      "Zone 224 at TimeBin 52\n",
      "Zone 79 at TimeBin 53\n",
      "Zone 224 at TimeBin 54\n",
      "Zone 79 at TimeBin 55\n",
      "Zone 107 at TimeBin 56\n",
      "Zone 79 at TimeBin 57\n",
      "Zone 107 at TimeBin 58\n",
      "Zone 79 at TimeBin 59\n",
      "Zone 224 at TimeBin 60\n",
      "Zone 79 at TimeBin 61\n",
      "Zone 107 at TimeBin 62\n",
      "Zone 79 at TimeBin 63\n",
      "Zone 107 at TimeBin 64\n",
      "Zone 79 at TimeBin 65\n",
      "Zone 107 at TimeBin 66\n",
      "Zone 224 at TimeBin 67\n",
      "Zone 79 at TimeBin 68\n",
      "Zone 224 at TimeBin 69\n",
      "Zone 79 at TimeBin 70\n",
      "Zone 224 at TimeBin 71\n",
      "Zone 79 at TimeBin 72\n",
      "Zone 107 at TimeBin 73\n",
      "Zone 224 at TimeBin 74\n",
      "Zone 79 at TimeBin 75\n",
      "\n",
      "ðŸ’° Total Estimated Income over 32 steps: $454.35\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_policy(df, rollout_fn, num_rollouts=100, steps=32):\n",
    "    \"\"\"\n",
    "    Evaluate average income over multiple rollouts from random starting points.\n",
    "    \n",
    "    Args:\n",
    "        df: The DataFrame with original training samples (for zone/time sampling)\n",
    "        rollout_fn: The rollout function\n",
    "        num_rollouts: Number of rollouts to simulate\n",
    "        steps: Steps per rollout (32 = 8 hours with 15min bin)\n",
    "    \n",
    "    Returns:\n",
    "        avg_reward: average total income over all rollouts\n",
    "        all_rewards: list of individual rollout incomes\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(num_rollouts):\n",
    "        row = df.sample(1).iloc[0]\n",
    "        s_zone = inv_zone_map[row['s_zone']]\n",
    "        t_bin = int(row['t_bin'])\n",
    "        \n",
    "        total_r, _ = rollout_fn(s_zone, t_bin, steps=steps)\n",
    "        rewards.append(total_r)\n",
    "\n",
    "    avg_reward = sum(rewards) / len(rewards)\n",
    "    std_dev = np.std(rewards)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Evaluated over {num_rollouts} random rollouts:\")\n",
    "    print(f\"â†’ Average income: ${avg_reward:.2f}\")\n",
    "    print(f\"â†’ Std deviation: Â±${std_dev:.2f}\")\n",
    "    return avg_reward, rewards\n",
    "evaluate_policy(df, rollout, num_rollouts=100, steps=32)"
   ],
   "id": "782ac1602d7fbe1d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
